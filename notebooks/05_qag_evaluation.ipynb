{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Question-Answer Generation (QAG) framework is a evaluation technique recently proposed to assess factual consistency of summaries. The initially proposed process starts with generating some questions based on the summary with LLMs then asks a LLM to answer these questions using both the summary and the original document. The more answers are the same, the better the summary is. \n",
    "\n",
    "Confident.ai builds summarisation metric following this process with two modifications.\n",
    "1. Make questions closed-ended so they can be answered by 'yes' or 'no' for easier scoring.\n",
    "2. Allow questions to be generated from either the summary or the source document. When questions are from the original text, the score measures **inclusion** of details; when questions are from the summary, the score measurs factual **aligment**. \n",
    "\n",
    "In this notebook, I experimented this technique with further adjustments, mostly by changing the prompts. \n",
    "1. Questions are still closed-ended but have to relate to the distinct and important information from the given text. \n",
    "2. Instead of calling a LLM to answer questions one at a time (which can be costly), answer all questions in one call. \n",
    "3. request quotes for the anwser to aid understanding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "sys.path.append('/mnt/d/Projects/papersurvey_tool/src/')\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the custom summariser for the summarisation task\n",
    "from summarisation.summariser import PaperSummariser\n",
    "file_path = \"../example_paper1.pdf\"\n",
    "autosum = PaperSummariser()\n",
    "final_summary = autosum.summarise(file_path)\n",
    "full_doc = \"\\n\".join(autosum.text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The research paper demonstrates that tree-based models consistently surpass neural networks in performance on tabular data due to their abilities to handle irregular target function patterns, disregard uninformative features, and correctly interpret non-rotationally invariant data. The findings also emphasized the difference in performance between the models could vary significantly based on dataset size, missing data, and high-cardinality categorical features.\n",
      "\n",
      "Findings: \n",
      "- Tree-based models greatly outperform neural networks in dealing with tabular data due to their ability to handle irregular data patterns and uninformative features.\n",
      "- There could be a variance in performance between neural networks and tree-based models concerning missing data and high cardinality categorical features.\n",
      "- The effectiveness of these models could be impacted by the size of the data set used.\n",
      "\n",
      "Methods: The study utilized numerical feature-based classification, performing Gaussianization of all features before random rotations. It employed averages across datasets and represented the distributions across random search shuffles visually.\n"
     ]
    }
   ],
   "source": [
    "print(final_summary['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(text, n=5):\n",
    "\n",
    "    closed_end_questions_template = \"\"\"\n",
    "    For the given text below, please follow the Guidance to generate {n} questions. \n",
    "    \n",
    "    Text: {text}\n",
    "\n",
    "    Guidance:\n",
    "    - questions should be closed-ended that can be answered by 'yes' or 'no'. \n",
    "    - questions should be related to the important facts of the text.\n",
    "    - use distinct information from different parts of the text to generate questions.\n",
    "    - Return only the questions in JSON as shown in the example output below.\n",
    "\n",
    "    Example Output: {{questions: [list of questions]}}\n",
    "\n",
    "    \"\"\"\n",
    "    prompt= closed_end_questions_template.format(n=n, text=text)\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"questions\": [ \"Do tree-based models still outperform deep learning on tabular data?\", \"Has deep learning enabled progress on text and image datasets?\", \"Was an empirical investigation conducted to understand the gap between tree-based models and Neural Networks?\", \"Did the study show that tree-based models remain state-of-the-art on medium-sized data?\", \"Did the researchers contribute a standard benchmark and raw data for baselines?\" ] }\n"
     ]
    }
   ],
   "source": [
    "questions = get_questions(text=full_doc, n=5)\n",
    "print(questions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(text, questions):\n",
    "\n",
    "    closed_end_answers_template = \"\"\"\n",
    "    You are given several questions separated by '\\n\\n' and a text. \n",
    "    Answer each question in 'yes', 'no', or 'idk'.\n",
    "    For each qusetion, find one or two quotes from the text that are most relevant to answering the question, then print them in numbered order. \n",
    "    Quotes should be reletively short. \n",
    "    Follow the example output to format your response.\n",
    "\n",
    "    If there are no relevant quotes, print 'no quotes found'.\n",
    "\n",
    "    Text: {text}\n",
    "\n",
    "    Questions: {questions}\n",
    "\n",
    "    \n",
    "    Example Output: [{{'question': question, 'answer': answer, 'quotes': [list of quotes]}}]\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    prompt = closed_end_answers_template.format(text=text, questions=questions)\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_str = \"\\n\\n\".join(eval(questions)['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'Do tree-based models still outperform deep learning on tabular data?', 'answer': 'yes', 'quotes': ['1. \"Tree-based models greatly outperform neural networks in dealing with tabular data due to their ability to handle irregular data patterns and uninformative features.\"', '2. \"The research paper demonstrates that tree-based models consistently surpass neural networks in performance on tabular data due to their abilities to handle irregular target function patterns, disregard uninformative features, and correctly interpret non-rotationally invariant data.\"']}, \n",
      "{'question': 'Has deep learning enabled progress on text and image datasets?', 'answer': 'idk', 'quotes': []}, \n",
      "{'question': 'Was an empirical investigation conducted to understand the gap between tree-based models and Neural Networks?', 'answer': 'yes', 'quotes': ['1. \"The study utilized numerical feature-based classification, performing Gaussianization of all features before random rotations.\"']}, \n",
      "{'question': 'Did the study show that tree-based models remain state-of-the-art on medium-sized data?', 'answer': 'idk', 'quotes': []}, \n",
      "{'question': 'Did the researchers contribute a standard benchmark and raw data for baselines?', 'answer': 'idk', 'quotes': []}]\n"
     ]
    }
   ],
   "source": [
    "qa_from_summary = get_answers(text=final_summary, questions=questions_str)\n",
    "print(qa_from_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'Do tree-based models still outperform deep learning on tabular data?', \n",
      "  'answer': 'yes', \n",
      "  'quotes': \n",
      "    ['1. \"While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear.\"',\n",
      "     '2. \"Results show that treebased models remain state-of-the-art on medium-sized data (∼10K samples) even without accounting for their superior speed.\"']}, \n",
      "    \n",
      " {'question': 'Has deep learning enabled progress on text and image datasets?', \n",
      "  'answer': 'yes', \n",
      "  'quotes': \n",
      "    ['1. \"While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear.\"']}, \n",
      "   \n",
      " {'question': 'Was an empirical investigation conducted to understand the gap between tree-based models and Neural Networks?', \n",
      "  'answer': 'yes', \n",
      "  'quotes': \n",
      "    ['1. \"To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs).\"',\n",
      "     '2. \"Our contributions are as follow: 3. We investigate empirically why tree-based models outperform deep learning, by ﬁnding data transformations which narrow or widen their performance gap.\"']}, \n",
      "   \n",
      " {'question': 'Did the study show that tree-based models remain state-of-the-art on medium-sized data?', \n",
      "  'answer': 'yes', \n",
      "  'quotes': \n",
      "    ['1. \"Results show that treebased models remain state-of-the-art on medium-sized data (∼10K samples) even without accounting for their superior speed.\"']}, \n",
      "   \n",
      " {'question': 'Did the researchers contribute a standard benchmark and raw data for baselines?', \n",
      "  'answer': 'yes', \n",
      "  'quotes': \n",
      "    ['1. \"To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.\"']}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_from_source = get_answers(text=full_doc, questions=questions_str)\n",
    "print(qa_from_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare answers and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(source_doc, summary, n):\n",
    "\n",
    "    questions = get_questions(text=source_doc, n=n)\n",
    "    questions_list = eval(questions)['questions']\n",
    "    questions_str = \"\\n\\n\".join(questions_list)\n",
    "\n",
    "    qa_source = get_answers(text=source_doc, questions=questions_str)\n",
    "    qa_source_df = pd.DataFrame(eval(qa_source))\n",
    "    qa_summary = get_answers(text=summary, questions=questions_str)\n",
    "    qa_summary_df = pd.DataFrame(eval(qa_summary))\n",
    "    comparison_df = pd.merge(qa_source_df, qa_summary_df, on='question', how='inner')\n",
    "    comparison_df.rename(columns={\"answer_x\": \"answer_source\", \"answer_y\": \"answer_summary\"}, inplace=True)\n",
    "    inclusion_score = sum(comparison_df[\"answer_source\"] == comparison_df[\"answer_summary\"]) / len(comparison_df)\n",
    "\n",
    "    return inclusion_score, comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusion_score, comparison_df = evaluate(source_doc=full_doc, summary=final_summary, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inclusion_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_source</th>\n",
       "      <th>quotes_x</th>\n",
       "      <th>answer_summary</th>\n",
       "      <th>quotes_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do tree-based models still outperform deep lea...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[1. Tree-based models remain state-of-the-art ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[1: \"Tree-based models greatly outperform neur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does tuning hyperparameters make Neural Networ...</td>\n",
       "      <td>no</td>\n",
       "      <td>[1. Tree-based models are superior for every r...</td>\n",
       "      <td>idk</td>\n",
       "      <td>[no quotes found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are Categorical variables considered the main ...</td>\n",
       "      <td>no</td>\n",
       "      <td>[1. Our results on numerical variables only do...</td>\n",
       "      <td>idk</td>\n",
       "      <td>[1: \"There could be a variance in performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are Neural Networks biased towards overly smoo...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[1. Such results suggest that the target funct...</td>\n",
       "      <td>idk</td>\n",
       "      <td>[no quotes found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do data rotations impact the performance of th...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[1. Fig. 6a, which shows the change in test ac...</td>\n",
       "      <td>idk</td>\n",
       "      <td>[no quotes found]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer_source  \\\n",
       "0  Do tree-based models still outperform deep lea...           yes   \n",
       "1  Does tuning hyperparameters make Neural Networ...            no   \n",
       "2  Are Categorical variables considered the main ...            no   \n",
       "3  Are Neural Networks biased towards overly smoo...           yes   \n",
       "4  Do data rotations impact the performance of th...           yes   \n",
       "\n",
       "                                            quotes_x answer_summary  \\\n",
       "0  [1. Tree-based models remain state-of-the-art ...            yes   \n",
       "1  [1. Tree-based models are superior for every r...            idk   \n",
       "2  [1. Our results on numerical variables only do...            idk   \n",
       "3  [1. Such results suggest that the target funct...            idk   \n",
       "4  [1. Fig. 6a, which shows the change in test ac...            idk   \n",
       "\n",
       "                                            quotes_y  \n",
       "0  [1: \"Tree-based models greatly outperform neur...  \n",
       "1                                  [no quotes found]  \n",
       "2  [1: \"There could be a variance in performance ...  \n",
       "3                                  [no quotes found]  \n",
       "4                                  [no quotes found]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refereces and useful resources\n",
    "\n",
    "1. [A Step-By-Step Guide to Evaluating an LLM Text Summarization Task](https://www.confident-ai.com/blog/a-step-by-step-guide-to-evaluating-an-llm-text-summarization-task)\n",
    "2. [Asking and Answering Questions to Evaluate the Factual Consistency of Summaries](https://arxiv.org/pdf/2004.04228.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
