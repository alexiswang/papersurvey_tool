{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I created a paper summariser directly calling LLMs without using existing frameworks such as langchain and llamaindex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY  = os.environ[\"OPENAI_API_KEY\"]\n",
    "GOOGLE_SEARCH_API_KEY = os.environ[\"GOOGLE_SEARCH_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "loader = PyPDFLoader(\"../../example_paper1.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=0)\n",
    "text = text_splitter.split_documents(documents)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction_prompt = \"\"\"\n",
    "    Here is a piece of text from a research paper. \n",
    "    Article:\n",
    "    {text_chunk}\n",
    "   ----\n",
    "    \n",
    "    Summarise the given text by following the Guidance below.\n",
    "    \n",
    "    Guidance:\n",
    "    - Summarise the the key conclusions in a single short paragraph of approx. 100 words.\n",
    "    - the summary should be highly dense ans concise yet self-contained. \n",
    "    - list relevant findings, observations and supportive arguments for the key conclusionsin in 3-5 bullet points. \n",
    "    - each finding has 1-2 sentences and be as close to the original text\n",
    "    - describe the methods in 3-5 short sentences\n",
    "\n",
    "    <format>\n",
    "    Summary: <conclusion>\n",
    "    Findings: <findings>\n",
    "    Methods: <methods>\n",
    "    </format>\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "role_prompt = \"\"\"You are a reseach assistant with the task to do literature review. You never make up any information that isn't in the literatures.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_summary = []\n",
    "# hardcode where the text input should stop to remove references and appendix\n",
    "for i in range(7):\n",
    "    text_chunk = text[i].page_content\n",
    "    prompt = instruction_prompt.format(text_chunk=text_chunk)\n",
    "    messages = [{\"role\": \"system\", \"content\": role_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model = \"gpt-4\", \n",
    "    )\n",
    "\n",
    "    text_chunk_summary.append(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summary: The study explores the ongoing superiority of tree-based models (like XGBoost and Random Forests) over deep learning methods in handling medium-sized tabular data, even with no consideration for their superior speed. The authors propose three challenge areas for researchers developing tabular-specific Neural Networks - robustness against uninformative features, data orientation preservation, and capacity to learn irregular functions. The paper also presents a benchmark and baseline data feasibility for future research in tabular data architecture.\\n\\nFindings: \\n- Deep learning shows exceptional capabilities with image, language, and audio data sets, but its superiority with tabular data remains uncertain.\\n- Tree-based models continue to outperform deep learning methods on medium-sized tabular data.\\n- There exist pivotal differences in the inductive biases of tree-based models and Neural Networks.\\n- Propagation of robustness against uninformative features, preservation of data orientation, and ability to learn irregular functions were identified as research challenges for future tabular-specific Neural Network development.\\n- The verified lack of a standard benchmark for tabular data learning further complicates the evaluation of newly-introduced deep learning methods.\\n\\nMethods: \\n- The authors examined both standard and novel deep learning methods along with tree-based models across numerous datasets and combinations of hyperparameters.\\n- The study utilized 45 datasets from diverse domains that possessed the obvious characteristics of tabular data.\\n- An elaborate benchmarking methodology was introduced and applied to account for both model fitting and finding optimal hyperparameters.\\n- The study evaluated recent deep learning model performance, which had not been independently assessed previously.\\n- The researchers engaged in empirical exploration to understand the differences in inductive biases between tree-based models and Neural Networks.',\n",
       " 'Summary: This research reveals that tree-based models maintain superior performance over deep learning algorithms in handling medium-sized tabular datasets. The investigation identifies differing biases of both tree-based models and deep learning algorithms, with the latter struggling with irregular patterns and rotation invariance, particularly with uninformative features. A newly created benchmark is intended to advance deep learning models for tabular data.\\n\\nFindings: \\n- Tree-based models outperform deep learning algorithms on medium-sized tabular data.\\n- Neural networks have difficulties in learning irregular patterns of the target function and are impacted by their rotation invariance.\\n- Many uninformative features present in tabular data are handled poorly by deep learning models.\\n- A new benchmark for tabular data has been created which provides a methodology for choosing and preprocessing a wide variety of representative datasets.\\n\\nMethods:\\n- The research compared deep learning models and tree-based models on generic tabular datasets. \\n- They employed a precise methodology for choosing and preprocessing various representative datasets, which were shared through OpenML. \\n- They performed extensive comparisons across multiple settings, whilst factoring in the cost of choosing hyperparameters. \\n- The researchers also conducted an empirical study to understand why tree-based models outperform their deep learning counterparts.\\n- These results were gathered through extensive random searches and empirically uncovering data transformations narrowing or widening the performance gap between the two types of models.',\n",
       " 'Summary: This study provides empirical evidence to propose possible reasons as to why tree-based models outperform neural networks (NNs) on tabular data. One suggestion is that multi-layer perceptrons (MLPs) are expressive enough for tabular data but likely suffer due to insufficient regularization. The research also includes the creation and utilization of a benchmark composed of 45 carefully selected reference tabular datasets from various domains.\\n\\nFindings:\\n- The paper presents the first empirical investigation regarding why tree-based models are superior to NNs on tabular data.\\n- According to Kadra et al. [2021a], employing dataset-specific combinations of 13 regularization techniques for MLPs can deliver state-of-the-art results.\\n- The authors argue that MLPs, while well-suited for tabular data, might struggle due to lack of appropriate regularization.\\n- The researchers developed a benchmark that consists of 45 tabular datasets from a range of domains.\\n\\nMethods:\\n- The paper explores why tree-based models perform better than NNs on tabular data via empirical investigation.\\n- A benchmark was compiled that consists of 45 tabular datasets sourced primarily from OpenML.\\n- Datasets were selected based on certain criteria to ensure the chosen data were not high dimensional, the features were heterogeneous, and the data was not too small among others.\\n- To maintain homogeneity of learning tasks and concentrate on tabular data-specific challenges, the paper excludes certain subproblems such as medium-sized training set, missing data, etc. from the analysis.',\n",
       " 'Summary: The study describes a benchmarking procedure for models with hyperparameter tuning that combines the variance of hyperparameter tuning with incremental model evaluations. The performance of the models is studied via multiple iterations of random search. All experiments and raw data are provided for further study. Results are aggregated across different datasets using a specific metric. The data preparation involved minimal manual preprocessing, resulting in a conclusion that tree-based models still outperform deep learning on tabular data.\\n\\nFindings:\\n- Hyperparameter tuning introduces uncontrolled variance, especially when the model evaluation budget is limited.\\n- Random search for hyper-parameter tuning is combined with increasingly high budgets of model evaluations to create the benchmark.\\n- The dominant performance of tree-based models was observed and is compared to the performance of deep models.\\n- Results were aggregated across datasets using the average distance to the minimum (ADTM) metric.\\n- A minimal preprocessing approach was used in data preparation for testing different models.\\n\\nMethods:\\n- A benchmarking procedure was used, sampling the variance of hyperparameter tuning and exploring increasingly high budgets of model evaluations.\\n- Random searches of approximately 400 iterations per dataset were conducted, with built-in default hyperparameters.\\n- Performance was assessed based on the number of random search iterations, and this process was repeated 15 times.\\n- Results were aggregated using a specific distance metric and renormalization method.\\n- For data preparation, the study applied only a few transformations like Gaussianized features, transformed regression targets, and OneHotEncoding for models which do not handle categorical variables natively.',\n",
       " \"Summary: The research paper provides an overview on benchmarking conducted on medium-sized datasets, with numerical-only features and both numerical and categorical features, considering several permutations of random search iterations. Two specific Deep Learning Transformer models, FT_Transformer and SAINT, were analysed due to their proven efficacy within tabular data.\\n\\nFindings:\\n- The benchmarking was done on two types of medium-sized datasets: one with only numerical features and the other with both numerical and categorical features.\\n- Benchmarking involved the use of random search iterations, shuffled 15 times to study model performance.\\n- FT_Transformer, a simple Transformer model with numerical and categorical features embedding, was chosen on its demonstrated competence against tree-based models in a previous study.\\n- SAINT, a Transformer model with embedded modules and inter-sample attention mechanism, had shown high performance in another past study. \\n\\nMethods: \\n- The benchmark tests were performed on two types of medium-sized datasets, one with only numerical features and the other with both numerical and categorical features.\\n- The models' performance test score, with different hyper-parameters, was determined at each random search iteration, repeated and averaged across 15 different shuffles of the random order.\\n- Post benchmarking, the best model's score on a validation set was generated.\\n- FT_Transformer and SAINT - two Deep learning models, were specifically utilized. These were known for their proven performance on tabular data in previous researches.\",\n",
       " \"Summary: This research suggests that tree-based models consistently outperform Neural Networks (NNs) on tabular data irrespective of hyperparameters tuning. The gap persists even when only numerical features are used, contradicting the common belief that categorical variables were a key weakness for NNs on tabular data. A crucial element of the superior performance of tree-based models appears to be their usage of decision trees as weak learners within ensemble methods.\\n\\nFindings: \\n- Standard hyperparameters tuning did not make NNs competitive with tree-based models, which maintained superior performance across multiple random search budgets.\\n- Each iteration of random search was generally slower for NNs than for tree-based models.\\n- Analyzing with numerical features only still revealed a significant performance gap, with tree-based models outperforming NNs.\\n- Bagging or boosting methods using decision trees as weak learners were identified as best methods for tabular data. \\n\\nMethods: \\n- The performance of different models was tested through a Gaussian Kernel smoother. \\n- The smoothing was done using ScikitLearn’s QuantileTransformer. \\n- Several transformations were applied to tabular datasets to study the difference in the generalization performance gap.\\n- The analysis was limited to numerical variables and classification tasks on medium-sized datasets.\\n- The study also involved smoothing the output with a Gaussian Kernel smoother with various length-scale values to study models' ability to learn irregular patterns.\",\n",
       " 'Summary: The research identifies that Neural Networks (NNs) struggle to fit irregular functions in datasets compared to tree-based models. They find that Multi-Layer Perceptron (MLP) architectures are less robust to uninformative features common in tabular datasets and that data, which are non-rotationally invariant, can impact the learning procedures. Regularization and careful optimization, however, may allow NNs to learn these patterns. \\n\\nFindings: \\n- NNs struggle to fit irregular functions, which are prevalent in the studied datasets, compared to tree-based models. Tree-based models are unbiased towards low-frequency functions unlike NNs.\\n- MLP-like architectures face difficulty in handling uninformative features common in tabular datasets, leading to their decreased performance when such features exist. \\n- Rotational invariance of data tends to pose a challenge for MLPs, suggesting that learning procedures should not be rotationally invariant. \\n\\nMethods: \\n- The study employed removal of features based on their importance, as ranked by a Random Forest, to study the robustness of different models to uninformative features. \\n- They further carried out tests by applying random rotations to the datasets to test rotational invariance of the models. \\n- The potential value of regularization and careful optimization in allowing NNs to learn irregular patterns was also explored.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The research identifies that Neural Networks (NNs) struggle to fit irregular functions in datasets compared to tree-based models. They find that Multi-Layer Perceptron (MLP) architectures are less robust to uninformative features common in tabular datasets and that data, which are non-rotationally invariant, can impact the learning procedures. Regularization and careful optimization, however, may allow NNs to learn these patterns. \n",
      "\n",
      "Findings: \n",
      "- NNs struggle to fit irregular functions, which are prevalent in the studied datasets, compared to tree-based models. Tree-based models are unbiased towards low-frequency functions unlike NNs.\n",
      "- MLP-like architectures face difficulty in handling uninformative features common in tabular datasets, leading to their decreased performance when such features exist. \n",
      "- Rotational invariance of data tends to pose a challenge for MLPs, suggesting that learning procedures should not be rotationally invariant. \n",
      "\n",
      "Methods: \n",
      "- The study employed removal of features based on their importance, as ranked by a Random Forest, to study the robustness of different models to uninformative features. \n",
      "- They further carried out tests by applying random rotations to the datasets to test rotational invariance of the models. \n",
      "- The potential value of regularization and careful optimization in allowing NNs to learn irregular patterns was also explored.\n"
     ]
    }
   ],
   "source": [
    "print(text_chunk_summary[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_chunk =\"\\n\".join(text_chunk_summary)\n",
    "\n",
    "prompt = instruction_prompt.format(text_chunk=text_chunk)\n",
    "messages = [{\"role\": \"system\", \"content\": role_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-4\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The studies indicate that tree-based models consistently outperform Neural Networks (NNs) in handling tabular data. This performance gap persists even with standard hyperparameters tuning and when only numerical features are used. The unusual performance advantage of tree-based models can be attributed to their use of decision trees within ensemble methods and their superior robustness to uninformative features common in tabular datasets.\n",
      "\n",
      "Findings: \n",
      "- Tree-based models maintain a higher level of performance over NNs across various random search budgets, even with hyperparameters tuning. \n",
      "- The superiority of tree-based models endures even when the datasets are limited to numerical variables only, going against the belief that NNs primarily struggle with categorical variables in tabular data.\n",
      "- Tree-based models handle uninformative features and irregular functions better than MLP-like Neural Network architectures.\n",
      "- Bagging or boosting methods that utilize decision trees as weak learners provide the best performance for tabular data.\n",
      "\n",
      "Methods: \n",
      "- Performance testing of the models involved iterative Gaussian Kernel smoothing with ScikitLearn’s QuantileTransformer, alongside random search iterations.\n",
      "- To determine the models' robustness to uninformative features, the researchers carried out feature importance ranking and subsequent feature removal using a Random Forest.\n",
      "- Random rotations were applied to the datasets to scrutinize the rotational invariance of the models.\n",
      "- A benchmarking approach was deployed for model comparison, utilizing different hyperparameters and an average distance to the minimum (ADTM) metric for results aggregation.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any\n",
    "# from serpapi import GoogleSearch\n",
    "# import regex as re\n",
    "# import prompts\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "# class PaperSummariser:\n",
    "\n",
    "#     def __init__(self) -> None:\n",
    "#         self.stop_at = 7\n",
    "#         self.PROMPTS = prompts\n",
    "\n",
    "#     def _summarise_content(self, text) -> Any:\n",
    "        \n",
    "#         text_chunk_summary = []\n",
    "#         for i in range(self.stop_at):\n",
    "#             text_chunk = text[i].page_content\n",
    "#             page_summary = self._summarise_page(text_chunk)\n",
    "#         text_chunk_summary.append(page_summary)\n",
    "\n",
    "#         all_page_summary =\"\\n\".join(text_chunk_summary)\n",
    "#         final_summary = self._summarise_page(all_page_summary)\n",
    "\n",
    "#         return final_summary\n",
    "\n",
    "#     def _summarise_page(self, text_chunk):\n",
    "        \n",
    "#         prompt = self.PROMPTS.INSTRUCTION_PROMPT.format(text_chunk=text_chunk)\n",
    "#         messages = [{\"role\": \"system\", \"content\": self.PROMPTS.ROLE_PROMPT},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}]\n",
    "#         client = OpenAI()\n",
    "#         response = client.chat.completions.create(\n",
    "#                     messages=messages,\n",
    "#                     model = \"gpt-4\", \n",
    "#         )\n",
    "#         return response.choices[0].message.content\n",
    "    \n",
    "#     def get_paper_info(self, text_chunk):\n",
    "\n",
    "#         prompt = self.PROMPTS.EXTRACTION_PROMPT.format(text_chunk=text_chunk)\n",
    "#         messages = [{\"role\": \"system\", \"content\": self.PROMPTS.ROLE_PROMPT},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}]\n",
    "#         client = OpenAI()\n",
    "#         response = client.chat.completions.create(\n",
    "#                 messages=messages,\n",
    "#                 model = \"gpt-4\", \n",
    "#         )\n",
    "#         info = eval(response.choices[0].message.content)\n",
    "\n",
    "#         return info\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def google_scholar_search(query):\n",
    "#         params = {\n",
    "#         \"engine\": \"google_scholar\",\n",
    "#         \"q\": query,\n",
    "#         \"api_key\": GOOGLE_SEARCH_API_KEY\n",
    "#     }\n",
    "#         search = GoogleSearch(params)\n",
    "#         results = search.get_dict()\n",
    "#         summary = results[\"organic_results\"][0][\"publication_info\"][\"summary\"]\n",
    "#         publish_year = re.search(r'(19|20)\\d\\d', summary).group(0)\n",
    "#         cited_times = results[\"organic_results\"][0][\"inline_links\"][\"cited_by\"][\"total\"]\n",
    "\n",
    "#         return publish_year, cited_times\n",
    "\n",
    "#     def preprocess(self, filepath):\n",
    "\n",
    "#         loader = PyPDFLoader(filepath)\n",
    "#         documents = loader.load()\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=0)\n",
    "#         text = text_splitter.split_documents(documents)\n",
    "\n",
    "#         return text\n",
    "    \n",
    "#     def summarise(self, paper_path):\n",
    "#         summary = {}\n",
    "#         text = self.preprocess(paper_path)\n",
    "\n",
    "#         front_page = text[0]\n",
    "#         info = self.get_paper_info(front_page)\n",
    "#         title = info[\"title\"]\n",
    "#         summary.update(info)\n",
    "\n",
    "#         publish_year, cited_times = PaperSummariser.google_scholar_search(query=title)\n",
    "#         summary.update({\"publish_year\": publish_year, \"cited_times\": cited_times})\n",
    "\n",
    "#         final_summary = self._summarise_content(text)\n",
    "#         summary.update({\"summary\": final_summary})\n",
    "\n",
    "#         return summary\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summariser import PaperSummariser\n",
    "automsum = PaperSummariser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = automsum.summarise(\"../../example_paper3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'SummEval: Re-evaluating Summarization Evaluation',\n",
       " 'authors': ['Alexander R. Fabbri',\n",
       "  'Wojciech Kry ´sci´nski',\n",
       "  'Bryan McCann',\n",
       "  'Caiming Xiong',\n",
       "  'Richard Socher',\n",
       "  'Dragomir Radev'],\n",
       " 'publish_year': '2021',\n",
       " 'cited_times': 416,\n",
       " 'summary': 'Summary: The research paper presents a versatile evaluation toolkit featuring 14 automatic evaluation metrics intended to simplify multi-metric evaluation and results processing. Accompanying this toolkit is a human evaluation system that utilizes a variety of neural summarization models. The evaluation process, \\nencompassing both expert and crowd-sourced judges, emphasizes on four dimensions: coherence, consistency, fluency, and relevance.\\n\\nFindings:\\n- Fourteen automatic evaluation metrics are consolidated in the proposed toolkit, intending to streamline the evaluation procedure.\\n- The evaluation process includes using crowd-sourced evaluators and experts to assess summaries from neural models.\\n- The evaluation process tests 100 articles and garners 12800 summary-level annotations.\\n- The evaluation revolves around four criteria: coherence, consistency, fluency, and relevance.\\n\\nMethods:\\n- The proposed evaluation metrics are applied on all example and corpus levels to ensure standardized evaluation.\\n- The toolkit includes a standard configuration that reflects popular metric settings for ease of use.\\n- Custom configurations are possible through external gin configuration files.\\n- The toolkit offers a command-line tool that facilitates parallel measures evaluation.\\n- With eight judges for each article, every dimension is effectively assessed and annotated.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The research paper discusses a Python package unifying 14 automatic evaluation metrics under an evaluation toolkit. It utilized human annotations from a pool of crowd-sourced judges and experts to evaluate summaries based on coherence, consistency, fluency, and relevance parameters leading to the evaluation of 16 neural summarization models.\n",
      "\n",
      "Findings:\n",
      "- The evaluation toolkit consists of 14 consolidated metrics with batch and example-level evaluation features.\n",
      "- It offers a consistent interface for multi-metric evaluations and customization options for each metric.\n",
      "- Over 12,800 human annotations were collected to provide insights into different summary parameters.\n",
      "- The collected annotations were applied to evaluate 16 different neural summarization models.\n",
      "\n",
      "Methods:\n",
      "- The research utilized a Python package-based evaluation toolkit with several automatic metrics.\n",
      "- It supported both batch and individual evaluations for each metric with custom configuration options.\n",
      "- Summaries were evaluated on the basis of coherence, consistency, fluency, and relevance for human annotations.\n",
      "- These annotations were sourced from crowd-sourced judges and experts to evaluate a series of neural summarization models.\n"
     ]
    }
   ],
   "source": [
    "print(summary[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = text_chunk_summary\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in writing rich and dense summaries in broad domains.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article:\n",
    "\n",
    "{article}\n",
    "\n",
    "----\n",
    "\n",
    "You will generate increasingly concise, entity-dense summaries of the above\n",
    "Article.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "- Step 1: Identify 1-3 informative Entities from the Article\n",
    "which are missing from the previously generated summary and are the most\n",
    "relevant.\n",
    "\n",
    "- Step 2: Write a new, denser summary of identical length which covers\n",
    "every entity and detail from the previous summary plus the missing entities\n",
    "\n",
    "A Missing Entity is:\n",
    "\n",
    "- Relevant: to the main story\n",
    "- Specific: descriptive yet concise (5 words or fewer)\n",
    "- Novel: not in the previous summary\n",
    "- Faithful: present in the Article\n",
    "- Anywhere: located anywhere in the Article\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "highly non-specific, containing little information beyond the entities\n",
    "marked as missing.\n",
    "\n",
    "- Use overly verbose language and fillers (e.g. \"this article discusses\") to\n",
    "reach approx. 80 words.\n",
    "\n",
    "- Make every word count: re-write the previous summary to improve flow and\n",
    "make space for additional entities.\n",
    "\n",
    "- Make space with fusion, compression, and removal of uninformative phrases\n",
    "like \"the article discusses\"\n",
    "\n",
    "- The summaries should become highly dense and concise yet self-contained,\n",
    "e.g., easily understood without the Article.\n",
    "\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "- Never drop entities from the previous summary. If space cannot be made,\n",
    "add fewer new entities.\n",
    "\n",
    "> Remember to use the exact same number of words for each summary.\n",
    "Answer in JSON.\n",
    "\n",
    "> The JSON in `summaries_per_step` should be a list (length 5) of\n",
    "dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-4\",   \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
