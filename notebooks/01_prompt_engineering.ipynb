{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I experiment different prompting techniques for summarisation tasks, with the aim to:\n",
    "\n",
    "- better understand how they work\n",
    "- craft appropriate prompts for summarising a research paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "I will be using a paper (in pdf) I recently read and leverage Langchain to load and split. As the paper is long, I will mainly use the first page but have also tested on other pages (chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper has 33 pages\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "loader = PyPDFLoader(\"../../example_paper1.pdf\")\n",
    "documents = loader.load()\n",
    "print(f\"This paper has {len(documents)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do tree-based models still outperform deep\n",
      "learning on tabular data?\n",
      "Léo Grinsztajn\n",
      "Soda, Inria Saclay\n",
      "leo.grinsztajn@inria.frEdouard Oyallon\n",
      "ISIR, CNRS, Sorbonne UniversityGaël Varoquaux\n",
      "Soda, Inria Saclay\n",
      "Abstract\n",
      "While deep learning has enabled tremendous progress on text and image datasets,\n",
      "its superiority on tabular data is not clear. We contribute extensive benchmarks of\n",
      "standard and novel deep learning methods as well as tree-based models such as\n",
      "XGBoost and Random Forests, across a large number of datasets and hyperparame-\n",
      "ter combinations. We deﬁne a standard set of 45 datasets from varied domains with\n",
      "clear characteristics of tabular data and a benchmarking methodology accounting\n",
      "for both ﬁtting models and ﬁnding good hyperparameters. Results show that tree-\n",
      "based models remain state-of-the-art on medium-sized data ( ∼10K samples) even\n",
      "without accounting for their superior speed. To understand this gap, we conduct an\n",
      "empirical investigation into the differing inductive biases of tree-based models and\n",
      "Neural Networks (NNs). This leads to a series of challenges which should guide\n",
      "researchers aiming to build tabular-speciﬁc NNs: 1.be robust to uninformative\n",
      "features, 2.preserve the orientation of the data, and 3.be able to easily learn\n",
      "irregular functions. To stimulate research on tabular architectures, we contribute a\n",
      "standard benchmark and raw data for baselines: every point of a 20 000 compute\n",
      "hours hyperparameter search for each learner.\n",
      "1 Introduction\n",
      "Deep learning has enabled tremendous progress for learning on image, language, or even audio\n",
      "datasets. On tabular data, however, the picture is muddier and ensemble models based on decision\n",
      "trees like XGBoost remain the go-to tool for most practitioners [Sta] and data science competitions\n",
      "[Kossen et al., 2021]. Indeed deep learning architectures have been crafted to create inductive biases\n",
      "matching invariances and spatial dependencies of the data. Finding corresponding invariances is hard\n",
      "in tabular data, made of heterogeneous features, small sample sizes, extreme values.\n",
      "Creating tabular-speciﬁc deep learning architectures is a very active area of research (see section 2)\n",
      "given that tree-based models are not differentiable, and thus cannot be easily composed and jointly\n",
      "trained with other deep learning blocks. Most corresponding publications claim to beat or match tree-\n",
      "based models, but their claims have been put into question: a simple Resnet seems to be competitive\n",
      "with some of these new models [Gorishniy et al., 2021], and most of these methods seem to fail on new\n",
      "datasets [Shwartz-Ziv and Armon, 2021]. Indeed, the lack of an established benchmark for tabular\n",
      "data learning provides additional degrees of freedom to researchers when evaluating their method.\n",
      "Furthermore, most tabular datasets available online are small compared to benchmarks in other\n",
      "machine learning subdomains, such as ImageNet [Ima], making evaluation noisier. These issues add\n",
      "up to other sources of unreplicability across machine learning, such as unequal hyperparameters tuning\n",
      "efforts [Lipton and Steinhardt, 2019] or failure to account for statistical uncertainty in benchmarks\n",
      "[Bouthillier et al., 2021]. To alleviate these concerns, we contribute a tabular data benchmark with a\n",
      "precise methodology for datasets inclusion and hyperparameter tuning. This enables us to evaluate\n",
      "recent deep learning models which have not yet been independently evaluated, and to show that\n",
      "Preprint. Under review.arXiv:2207.08815v1  [cs.LG]  18 Jul 2022\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=0)\n",
    "text = text_splitter.split_documents(documents)\n",
    "text_chunk = text[0].page_content\n",
    "print(text_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = f\"\"\"summarise the text in {text_chunk}\"\"\"\n",
    "role_prompt = \"\"\"You are a reseach assistant with the task to do literature review. You never make up any information that isn't in the literatures.\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": role_prompt},\n",
    "            {\"role\": \"user\", \"content\": basic_prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-3.5-turbo\",   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the performance of tree-based models compared to deep learning models on tabular data. While deep learning models have been successful in text and image datasets, their superiority in tabular data is not clear. The authors conduct extensive benchmarks on various datasets and hyperparameter combinations, comparing deep learning methods and tree-based models such as XGBoost and Random Forests. The results show that tree-based models are still state-of-the-art on medium-sized data, even without considering their faster speed. The authors then investigate the differences in the inductive biases of tree-based models and neural networks, highlighting the challenges in building tabular-specific neural networks. To stimulate research in this area, the authors provide a standard benchmark and raw data for baselines.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chain of thought (COT)\n",
    "\n",
    "The idea of cot to enable complex reasoning capabilities through intermediate reasoning steps. \n",
    "\n",
    "Zero-shot cot can be achieved by simplying add \"Let's think step by step\" to the orignal prompt. This is more likely to be helpful for math problems and other symbolic reasoning problem, but I still want to give it try to see how it works for this task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text[0].page_content\n",
    "instruction_prompt = f\"\"\"Here is a piecec of document from a research paper you will summarise: {text_chunk}\n",
    "\n",
    "Keep the summary short.\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "role_prompt = \"\"\"You are a reseach assistant with the task to do literature review. You never make up any information that isn't in the literatures.\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": role_prompt},\n",
    "            {\"role\": \"user\", \"content\": instruction_prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-3.5-turbo\",   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper titled \"Why do tree-based models still outperform deep learning on tabular data?\" by Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux discusses the performance of deep learning models versus tree-based models on tabular data.\n",
      "\n",
      "The authors conduct extensive benchmarks on standard and novel deep learning methods, as well as tree-based models like XGBoost and Random Forests. They use a standard set of 45 datasets with clear characteristics of tabular data and a benchmarking methodology that accounts for both model fitting and finding good hyperparameters.\n",
      "\n",
      "The results show that tree-based models continue to be state-of-the-art on medium-sized data, even without considering their superior speed. The authors also investigate the differing inductive biases of tree-based models and neural networks (NNs) through an empirical investigation.\n",
      "\n",
      "Based on their findings, the authors highlight the challenges that researchers aiming to build tabular-specific NNs should address, including robustness to uninformative features, preservation of data orientation, and the ability to easily learn irregular functions.\n",
      "\n",
      "To encourage research on tabular architectures, the authors provide a standard benchmark and raw data for baselines, which includes a 20,000 compute hours hyperparameter search for each learner.\n",
      "\n",
      "In summary, this paper concludes that tree-based models currently outperform deep learning models on tabular data, and provides insights into the challenges that need to be addressed in order to develop effective tabular-specific neural network architectures.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COT (few-shot)\n",
    "\n",
    "Another technique often mentioned along with zero-shot is few-shot, which involves providing a few examples in the prompt to \"teach\" the model to reason. However, I found it less useful(practical) for a summarisation task, because unlike math calculations or answering specific questions, it is hard to come up with some (good) summaries. It is also difficult to define what is good in this context. Hence I skipped this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COT (steps + formating)\n",
    "\n",
    "Since the idea of COT is to break down the reasoning process to steps. I tried eplicitly writing down what the steps should be and added an example to specify the format I want. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text[0].page_content\n",
    "instruction_prompt = f\"\"\"Here is a piece of document from a research paper you will summarise: {text_chunk}\n",
    "\n",
    "Take the folllowing steps.\n",
    "\n",
    "First, find the authors of the paper and the titile of the paper. If no information can be found, write \"not known\" instead.\n",
    "\n",
    "Then, summarise the key findings and the techniques used to derive the findings in 3-5 sentences. \n",
    "\n",
    "Thus, the format of the response should be in JSON as what's shown in the <example></example> tags. Make sure to follow the fommating exactly.\n",
    "\n",
    "<example>\n",
    "{{\n",
    "    \"author\": author,\n",
    "    \"title\": title,\n",
    "    \"summary\": summary\n",
    "}}\n",
    "</example>\n",
    "\n",
    "\"\"\"\n",
    "role_prompt = \"\"\"You are a reseach assistant with the task to do literature review. You never make up any information that isn't in the literatures.\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": role_prompt},\n",
    "            {\"role\": \"user\", \"content\": instruction_prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-3.5-turbo\",   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"author\": \"Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux\",\n",
      "    \"title\": \"Why do tree-based models still outperform deep learning on tabular data?\",\n",
      "    \"summary\": \"The authors conducted extensive benchmarks comparing tree-based models such as XGBoost and Random Forests with standard and novel deep learning methods on a set of 45 tabular datasets. They found that tree-based models remained state-of-the-art on medium-sized data even without accounting for their superior speed. The authors conducted an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs) and identified challenges for building tabular-specific NNs. They contributed a standard benchmark and raw data for baselines to stimulate research on tabular architectures.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-consistency COT\n",
    "\n",
    "Another technique I encountered is self-consistency. The diea is to sample multple, diverse reasoning paths through few-shot COT and use the generations to select the most consistent answer (majority vote). It has shown to boost the performance of COT on tasks involving arithmetic and commonsense reasoning. \n",
    "\n",
    "Since I decided to not use few-shot, the implementation of self-consistency is to ask the model to follow the given steps n times. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text[0].page_content\n",
    "self_consistency_prompt = f\"\"\"\n",
    "Imagine three completely independent research assistants who will summarise a given piece of paper {text_chunk}.\n",
    "\n",
    "Each assistant follows the below 2 steps. \n",
    "\n",
    "1. find the authors of the paper and the titile of the paper. If no information can be found, write \"not known\" instead.\n",
    "\n",
    "2. summarise the key findings and the techniques used in a short paragraph. \n",
    "\n",
    "Each assistant should provide response in JSON following the format given in the <example></example> tags. \n",
    "\n",
    "<example>\n",
    "{{\n",
    "    \"author\": author,\n",
    "    \"title\": title,\n",
    "    \"summary\": <the author> <key findings> <supportive arguments>\n",
    "}}\n",
    "</example>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "role_prompt = \"\"\"You are a reseach assistant with the task to do literature review. You never make up any information that isn't in the literatures.\"\"\"\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"user\", \"content\": self_consistency_prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-3.5-turbo\",   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"assistant1\": {\n",
      "        \"author\": \"Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux\",\n",
      "        \"title\": \"Why do tree-based models still outperform deep learning on tabular data?\",\n",
      "        \"summary\": \"The authors conducted extensive benchmarks of deep learning methods and tree-based models on a variety of tabular datasets. They found that tree-based models, such as XGBoost and Random Forests, remain state-of-the-art on medium-sized data, even without considering their superior speed. The authors also investigated the differing inductive biases of tree-based models and Neural Networks (NNs) and identified challenges for building tabular-specific NNs. They provide a standard benchmark and raw data for baseline comparisons.\"\n",
      "    },\n",
      "    \"assistant2\": {\n",
      "        \"author\": \"Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux\",\n",
      "        \"title\": \"Why do tree-based models still outperform deep learning on tabular data?\",\n",
      "        \"summary\": \"The paper by Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux explores the superiority of tree-based models over deep learning on tabular data. They conducted extensive benchmarks on various datasets and found that tree-based models, such as XGBoost and Random Forests, remain the go-to tool for practitioners and data science competitions. The authors also highlight the challenges of creating tabular-specific deep learning architectures and provide a standard benchmark and raw data for further research in this area.\"\n",
      "    },\n",
      "    \"assistant3\": {\n",
      "        \"author\": \"Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux\",\n",
      "        \"title\": \"Why do tree-based models still outperform deep learning on tabular data?\",\n",
      "        \"summary\": \"The paper written by Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux addresses the question of why tree-based models outperform deep learning on tabular data. They conducted extensive benchmarks on standard and novel deep learning methods as well as tree-based models like XGBoost and Random Forests. The results showed that tree-based models remain state-of-the-art on medium-sized data, even without accounting for their superior speed. The authors also conducted an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs) and identified challenges for building tabular-specific NNs. They provide a standard benchmark and raw data for baseline comparisons and encourage further research in tabular architectures.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of density (COD)\n",
    "\n",
    "Chain of density prompting is a new technique recently developed specifically for summarisation tasks. Summaries are generated by iteratively incorporating missing salient entities from the source text without increasing the length. The prompt is taken from the original paper. I also used gpt-4 as suggested from the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunk = ''\n",
    "# for i in range(6):\n",
    "#     text_chunk += text[i].page_content\n",
    "text_chunk = text[0].page_content\n",
    "article = text_chunk\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in writing rich and dense summaries in broad domains.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article:\n",
    "\n",
    "{article}\n",
    "\n",
    "----\n",
    "\n",
    "You will generate increasingly concise, entity-dense summaries of the above\n",
    "Article.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "- Step 1: Identify 1-3 informative Entities from the Article\n",
    "which are missing from the previously generated summary and are the most\n",
    "relevant.\n",
    "\n",
    "- Step 2: Write a new, denser summary of identical length which covers\n",
    "every entity and detail from the previous summary plus the missing entities\n",
    "\n",
    "A Missing Entity is:\n",
    "\n",
    "- Relevant: to the main story\n",
    "- Specific: descriptive yet concise (5 words or fewer)\n",
    "- Novel: not in the previous summary\n",
    "- Faithful: present in the Article\n",
    "- Anywhere: located anywhere in the Article\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "highly non-specific, containing little information beyond the entities\n",
    "marked as missing.\n",
    "\n",
    "- Use overly verbose language and fillers (e.g. \"this article discusses\") to\n",
    "reach approx. 80 words.\n",
    "\n",
    "- Make every word count: re-write the previous summary to improve flow and\n",
    "make space for additional entities.\n",
    "\n",
    "- Make space with fusion, compression, and removal of uninformative phrases\n",
    "like \"the article discusses\"\n",
    "\n",
    "- The summaries should become highly dense and concise yet self-contained,\n",
    "e.g., easily understood without the Article.\n",
    "\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "- Never drop entities from the previous summary. If space cannot be made,\n",
    "add fewer new entities.\n",
    "\n",
    "> Remember to use the exact same number of words for each summary.\n",
    "Answer in JSON.\n",
    "\n",
    "> The JSON in `summaries_per_step` should be a list (length 5) of\n",
    "dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-4\",   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "\"summaries_per_step\": [\n",
      "{\n",
      "\"missing_entities\": [\"tabular data\", \"tree-based models\", \"deep learning\"],\n",
      "\"denser_summary\": \"The article discusses the use of deep learning and tree-based models like XGBoost and Random Forests on tabular data. It highlights that despite the advancements in deep learning, it is still not evidently superior to tree-based models when it comes to tabular data. The researchers also present benchmarks and challenges linked with the use of deep learning methods on tabular data, implying that the space still has plenty of room for research and development.\"\n",
      "},\n",
      "{\n",
      "\"missing_entities\": [\"medium-sized data\", \"inductive biases\", \"Neural Networks (NNs)\"],\n",
      "\"denser_summary\": \"The article emphasizes that tree-based models, such as XGBoost and Random Forests, continue to outperform deep learning, notably Neural Networks (NNs), on tabular data, especially with medium-sized samples. The discussion also centers around the inductive biases of tree-based models versus NNs and presents an empirical understanding of the differential performance. These biases are highlighted as key research areas for improving deep learning techniques for tabular data.\"\n",
      "},\n",
      "{\n",
      "\"missing_entities\": [\"datasets from varied domains\", \"tabular-specific deep learning architectures\", \"hyperparameters\"],\n",
      "\"denser_summary\": \"The study evaluates use of deep learning techniques versus tree-based models like XGBoost and Random Forests on a standard set of 45 datasets from varied domains with tabular data. It probes the challenges with the biases of these methods and their performance on medium-sized samples. Additionally, tabular-specific deep learning architectures and the role of finding good hyperparameters are identified as key research fields for advancing deep learning usage in tabular data.\"\n",
      "},\n",
      "{\n",
      "\"missing_entities\": [\"robust to uninformative features\", \"preserve the orientation of the data\", \"easily learn irregular functions\"],\n",
      "\"denser_summary\": \"The research compares deep learning and tree-based models such as XGBoost and Random Forests across 45 varied domain datasets. It uncovers that these tree models outclass their deep learning counterparts in handling medium-sized tabular data. The paper identifies the need for NNs to be robust to uninformative features, preserve orientation of data, and easily learn irregular functions, as part of efforts to build better deep architectures for tabular data.\"\n",
      "},\n",
      "{\n",
      "\"missing_entities\": [\"benchmarking methodology\", \"20 000 compute hours hyperparameter search\", \"statistical uncertainty in benchmarks\"],\n",
      "\"denser_summary\": \"Tree-based models outshine deep learning techniques like NNs in processing medium-sized tabular data, a conclusion drawn from analysis of 45 datasets. The research identifies that NNs should be better equipped in handling uninformative features, data orientations, and irregular functions. Benchmarking methodology incorporating hyperparameter search through a 20 000 compute hours strategy and accounting for statistical uncertainty in benchmarks is used to facilitate accurate evaluations and stimulate research in the field.\"\n",
      "}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text[6].page_content\n",
    "instruction_prompt = f\"\"\"\n",
    "    Here is a piece of text from a research paper. \n",
    "    Article:\n",
    "    {text_chunk}\n",
    "   ----\n",
    "    \n",
    "    Summarise the given text by following the Guidance below.\n",
    "    \n",
    "    Guidance:\n",
    "    - Summarise the the key conclusions in a single short paragraph of approx. 100 words.\n",
    "    - the summary should be highly dense ans concise yet self-contained. \n",
    "    - list relevant findings, observations and supportive arguments for the key conclusionsin in 3-5 bullet points. \n",
    "    - each finding has 1-2 sentences and be as close to the original text\n",
    "    - describe the methods in 3-5 short sentences\n",
    "\n",
    "    <format>\n",
    "    Summary: <conclusion>\n",
    "    Findings: <findings>\n",
    "    Methods: <methods>\n",
    "    </format>\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "role_prompt = \"\"\"You are a reseach assistant with the task to do literature review. You never make up any information that isn't in the literatures.\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": role_prompt},\n",
    "            {\"role\": \"user\", \"content\": instruction_prompt}]\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model = \"gpt-3.5-turbo\", \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The research paper explores the performance of neural networks (NNs) compared to tree-based models on tabular datasets. The study finds that NNs struggle to fit irregular functions compared to tree-based models, which learn piece-wise constant functions. The paper also highlights that uninformative features affect MLP-like NNs more than other models. Additionally, the research shows that data are non-invariant by rotation, highlighting the importance of learning procedures that are not rotationally invariant.\n",
      "\n",
      "Findings:\n",
      "- NNs struggle to fit irregular functions compared to tree-based models.\n",
      "- Uninformative features have a greater impact on MLP-like NNs compared to other models.\n",
      "- Data are non-invariant by rotation, suggesting the need for learning procedures that are not rotationally invariant.\n",
      "\n",
      "Methods:\n",
      "- The study uses tabular datasets and compares the performance of NNs and tree-based models.\n",
      "- Feature importance, ranked by a Random Forest, is used to drop uninformative features from the datasets.\n",
      "- The impact of removing informative and uninformative features on the performance of MLPs and other models is evaluated.\n",
      "- Random rotations are applied to the datasets to assess the rotation invariance of the models.\n",
      "- The paper references previous studies for additional support and context.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "1. Prompting is important. I once read a post on Linkedin saying prompting engineering is \"dead\" given the rise of RAGs and more capable models. Although it's debatable and may be the case with how rapidly technology evolves, I feel it still has an important role to play. \n",
    "\n",
    "2. The performance of different prompting techniques depends on the problem at hand. No single off-the-shelf prompt template works well across all applications. \n",
    "\n",
    "3. A good prompt (or a good prompting approach) requries logical thinking and creativity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful materials\n",
    "\n",
    "1. [ANTHROPIC - documentation](https://docs.anthropic.com/claude/docs/optimizing-your-prompt)\n",
    "2. [ANTHROPIC cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/long_context)\n",
    "3. [Summarising Best Practices for Prompt Engineering](https://towardsdatascience.com/summarising-best-practices-for-prompt-engineering-c5e86c483af4)\n",
    "4. [12 Prompt Engineering Techniques](https://cobusgreyling.medium.com/12-prompt-engineering-techniques-644481c857aa)\n",
    "5. [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "6. [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
