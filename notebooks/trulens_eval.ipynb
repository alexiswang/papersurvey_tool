{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "loader = PyPDFLoader(\"../../example_paper1.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "text = text_splitter.split_documents(documents)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(documents=text, \n",
    "                           embedding=embeddings)\n",
    "print(db._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "COHERE_API_KEY = os.environ[\"COHERE_API_KEY\"] \n",
    "\n",
    "# from typing import ForwardRef\n",
    "# from pydantic import BaseModel\n",
    "# from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# class CustomCohereRerank(CohereRerank):\n",
    "#     class Config(BaseModel.Config):\n",
    "#         arbitrary_types_allowed = True\n",
    "# co = Client()\n",
    "# CustomCohereRerank.update_forward_refs()\n",
    "\n",
    "# compressor = CustomCohereRerank(client=co)\n",
    "# retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\":4})\n",
    "# compressor = CohereRerank(user_agent=\"my-app\")\n",
    "# compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\", \n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "question = \"what are the key methods for building trustworthy AI?\"\n",
    "# result = qa_chain({\"query\": question})\n",
    "# print(result)\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "# compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import prompts\n",
    "\n",
    "\n",
    "class Langchain_app:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.docs = None\n",
    "        self.db = None\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        loader = PyPDFLoader(filepath)\n",
    "        self.docs = loader.load()\n",
    "        return self\n",
    "    \n",
    "    def indexing(self):\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "        text = text_splitter.split_documents(self.docs)\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        db = Chroma.from_documents(documents=text, \n",
    "                                embedding=embeddings)\n",
    "        self.db = db\n",
    "        return self\n",
    "    \n",
    "    def retrieving(self):\n",
    "        \n",
    "        retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\":4})\n",
    "        self.retriever = retriever\n",
    "        return self\n",
    "    \n",
    "    def responding(self, query):\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=self.retriever,\n",
    "            chain_type=\"stuff\", \n",
    "            return_source_documents=True\n",
    "        )\n",
    "        self.query_engine = qa_chain\n",
    "        response = qa_chain(query)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def summarise(self, filepath):\n",
    "\n",
    "        self.load_data(filepath)\n",
    "        self.indexing()\n",
    "        self.retrieving()\n",
    "\n",
    "        query = \"What are the key findings disucssed? Please summarise them in bullet points.\"\n",
    "        response = self.responding(query)\n",
    "        return response\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag1 = Langchain_app()\n",
    "response = rag1.summarise(\"../../example_paper1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the key findings disucssed? Please summarise them in bullet points.',\n",
       " 'result': '- The study explores the performance of tree-based models on tabular data.\\n- The study raises questions about other inductive biases of tree-based models that contribute to their performance on tabular data.\\n- The evaluation of tree-based models on small and large datasets, as well as datasets with missing data, is left as future work.\\n- The study reveals that tree-based models easily yield good predictions on tabular data with less computational cost.\\n- The study includes a figure showing the test accuracy of a Gradient Boosting Tree (GBT) for varying proportions of removed features, indicating the importance of features in the prediction accuracy.',\n",
       " 'source_documents': [Document(page_content='other ways to break rotation invariance which might be less computationally costly than embeddings.\\n6 Discussion and conclusion\\nLimitation Our study leaves open questions for future work: which other inductive biases of\\ntree-based models explain their performances on tabular data? How would our evaluation change on\\nvery small datasets? On very large datasets? What is the best way to handle speciﬁc challenges like\\nmissing data or high-cardinality categorical features, for NNs and tree-based models? With these best\\nmethods, how would the evaluation change including missing data?\\nConclusion While each publication on learning architectures for tabular data comes to different\\nresults using a different benchmarking methodology, our systematic benchmark, going beyond the\\nspeciﬁcities of a handful of datasets and accounting for hyper-parameter choice, reveals clear trends.\\nOn such data, tree-based models more easily yield good predictions, with much less computational\\n8', metadata={'page': 7, 'source': '../../example_paper1.pdf'}),\n",
       "  Document(page_content='Figure 21: Test accuracy of a GBT for varying proportions of removed features . Features are\\nremoved in increasing order of feature importance (computed with a Random Forest), and the two\\nlines correspond to the accuracy using the (most important) kept features (blue) or the (least important)\\nremoved features (red). Scores are normalized between 0 (random chance) and 1 (best score among\\nall hyperparameters). These scores are averaged across 30 random search orders, and the ribbons\\ncorrespond to the minimum and maximum values among these 30 orders. Same experiment than Fig.\\n4, shown for each dataset. Note that axes do not always start at zero.\\n28', metadata={'page': 27, 'source': '../../example_paper1.pdf'})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The study explores the performance of tree-based models on tabular data.\n",
      "- The study raises questions about other inductive biases of tree-based models that contribute to their performance on tabular data.\n",
      "- The evaluation of tree-based models on small and large datasets, as well as datasets with missing data, is left as future work.\n",
      "- The study reveals that tree-based models easily yield good predictions on tabular data with less computational cost.\n",
      "- The study includes a figure showing the test accuracy of a Gradient Boosting Tree (GBT) for varying proportions of removed features, indicating the importance of features in the prediction accuracy.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study conducted extensive benchmarks comparing tree-based models and deep learning models on tabular data. Tree-based models, such as XGBoost and Random Forests, outperformed deep learning models on medium-sized datasets. The study investigated the inductive biases of tree-based models and neural networks, highlighting challenges in building tabular-specific neural networks. The authors provided a standard benchmark and raw data to stimulate research in this area. The study also identified limitations of deep learning models in learning irregular patterns and rotation invariance. The authors hope their findings will assist in building successful deep learning models for tabular data. The study included a comprehensive benchmark with 45 datasets and addressed the lack of a standard benchmark in the field. The authors also conducted experiments on large-scale datasets and found that increasing the train set size reduced the performance gap between neural networks and tree-based models. The study provided insights into the benefits of using embeddings and the limitations of rotationally invariant algorithms in tabular data analysis. The authors also discussed the importance of breaking rotation invariance to improve performance. The study included a benchmarking procedure for models with hyperparameter selection and provided code and raw data for reproducibility. The authors compared the performance of different machine learning models and analyzed the importance of hyperparameters. The study also provided information on various datasets and references to related research papers.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "print(chain.run(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "from utils import get_prebuilt_trulens_recorder_langchain\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "# eval_questions = [\"who is the author of this article?\", \"what technique does this paper use?\"]\n",
    "\n",
    "# def run_evals(eval_questions, tru_recorder, query_engine):\n",
    "#     for question in eval_questions:\n",
    "#         with tru_recorder as recording:\n",
    "#             response = query_engine(question)\n",
    "\n",
    "\n",
    "# tru_recorder_langchain = get_prebuilt_trulens_recorder_langchain(\n",
    "#     qa_chain,\n",
    "#     app_id = \"Langchain Query Engine\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import (\n",
    "    Feedback,\n",
    "    TruLlama,\n",
    "    OpenAI,\n",
    "    TruChain,\n",
    "    Select\n",
    ")\n",
    "import numpy \n",
    "from trulens_eval.feedback import Groundedness\n",
    "openai = OpenAI()\n",
    "\n",
    "qa_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "qs_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=openai)\n",
    "\n",
    "groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "        .on(Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content)\n",
    "        .on_output()\n",
    "        .aggregate()\n",
    ")\n",
    "\n",
    "truchain = TruChain(\n",
    "        rag1.query_engine,\n",
    "        app_id='Chain1_ChatApplication',\n",
    "        feedbacks=[qa_relevance, qs_relevance, groundedness]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content(rag1.query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the key pillars for trustworthy AI?\"\n",
    "with truchain as recording:\n",
    "    rag1(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0][\"Groundedness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"Chain1_ChatApplication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder_langchain, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
